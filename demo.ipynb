{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DDFA_V2 Interactive Demo\n",
    "\n",
    "This notebook demonstrates how to use the 3DDFA_V2 (Three-D Dense Face Alignment Version 2) library for 3D face reconstruction from 2D images.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Face Detection**: How to find faces in images using FaceBoxes\n",
    "2. **3D Reconstruction**: How to build 3D face models from 2D images\n",
    "3. **Visualization**: Different ways to display the results (landmarks, meshes, depth maps)\n",
    "4. **Performance**: How to use ONNX for faster inference\n",
    "\n",
    "## Key Concepts:\n",
    "- **3DMM (3D Morphable Model)**: Mathematical model representing faces as combinations of shape and expression\n",
    "- **Parameters**: 62 numbers that describe any face (pose + shape + expression)\n",
    "- **Vertices**: 3D coordinates of points on the face surface\n",
    "- **Dense vs Sparse**: Dense = full mesh (~38k points), Sparse = landmarks (~68 points)\n",
    "\n",
    "## A simple demostration of how to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Before running this notebook, make sure you've built the required modules:\n",
    "# Run this in terminal: sh build.sh\n",
    "\n",
    "# === CORE COMPUTER VISION LIBRARIES ===\n",
    "import cv2                    # OpenCV - for image processing (reading, displaying, color conversion)\n",
    "import yaml                   # For loading configuration files (model settings, paths, etc.)\n",
    "\n",
    "# === 3DDFA_V2 CORE MODULES ===\n",
    "from FaceBoxes import FaceBoxes           # Face detector - finds rectangular boxes around faces\n",
    "from TDDFA import TDDFA                   # Main 3D face alignment engine - converts 2D faces to 3D models\n",
    "from utils.functions import draw_landmarks # Helper function to visualize facial landmarks\n",
    "from utils.render import render           # 3D mesh renderer - draws 3D face models on images  \n",
    "from utils.depth import depth             # Depth map generator - shows face depth information\n",
    "\n",
    "# === VISUALIZATION LIBRARY ===\n",
    "import matplotlib.pyplot as plt  # For displaying images and plots in the notebook\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to perform 3D face reconstruction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: LOAD CONFIGURATION ===\n",
    "# The config file contains important settings like:\n",
    "# - Model architecture (MobileNet, ResNet, etc.)\n",
    "# - Input image size (120x120 pixels)\n",
    "# - Model file paths and parameters\n",
    "# - 3DMM (3D Morphable Model) settings\n",
    "cfg = yaml.load(open('configs/mb1_120x120.yml'), Loader=yaml.SafeLoader)\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model architecture: {cfg.get('arch', 'Unknown')}\")\n",
    "print(f\"Input size: {cfg.get('size', 'Unknown')}x{cfg.get('size', 'Unknown')} pixels\")\n",
    "\n",
    "# === STEP 2: INITIALIZE MODELS ===\n",
    "# We can use either ONNX (faster) or PyTorch (standard) models\n",
    "onnx_flag = True  # Set to True for faster inference, False for standard PyTorch\n",
    "\n",
    "if onnx_flag:\n",
    "    print(\"\\nüöÄ Using ONNX optimized models (faster inference)\")\n",
    "    \n",
    "    # Set environment variables for optimal CPU performance\n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # Avoid OpenMP library conflicts\n",
    "    os.environ['OMP_NUM_THREADS'] = '4'          # Use 4 CPU threads for parallel processing\n",
    "    \n",
    "    # Import optimized ONNX versions\n",
    "    from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "    from TDDFA_ONNX import TDDFA_ONNX\n",
    "    \n",
    "    # Initialize the models\n",
    "    face_boxes = FaceBoxes_ONNX()    # Fast face detector (~1.5ms per image)\n",
    "    tddfa = TDDFA_ONNX(**cfg)        # Fast 3D face alignment (~1.35ms per face)\n",
    "    \n",
    "    print(\"‚úÖ ONNX models loaded - ready for fast inference!\")\n",
    "else:\n",
    "    print(\"\\nüêå Using standard PyTorch models\")\n",
    "    \n",
    "    # Initialize standard models\n",
    "    tddfa = TDDFA(gpu_mode=False, **cfg)  # 3D face alignment (CPU mode)\n",
    "    face_boxes = FaceBoxes()              # Face detection\n",
    "    \n",
    "    print(\"‚úÖ PyTorch models loaded - ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: LOAD TEST IMAGE ===\n",
    "# Load an example image to test face detection and 3D reconstruction\n",
    "img_fp = 'examples/inputs/emma.jpg'  # Path to test image\n",
    "img = cv2.imread(img_fp)             # Load image using OpenCV (BGR format)\n",
    "\n",
    "print(f\"Image loaded: {img_fp}\")\n",
    "print(f\"Image dimensions: {img.shape}\")  # Shows (height, width, channels)\n",
    "print(f\"Image type: {img.dtype}\")        # Should be uint8 (0-255 pixel values)\n",
    "\n",
    "# Display the image (convert BGR to RGB for matplotlib)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img[..., ::-1])  # Convert BGR to RGB for correct colors\n",
    "plt.title(\"Input Image for 3D Face Reconstruction\")\n",
    "plt.axis('off')             # Hide axes for cleaner display\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Image loaded and displayed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces using FaceBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: DETECT FACES IN THE IMAGE ===\n",
    "# FaceBoxes will scan the image and return bounding boxes around detected faces\n",
    "\n",
    "print(\"üîç Detecting faces in the image...\")\n",
    "boxes = face_boxes(img)  # Run face detection on the image\n",
    "\n",
    "print(f\"‚úÖ Detection complete!\")\n",
    "print(f\"Number of faces detected: {len(boxes)}\")\n",
    "\n",
    "# Display information about each detected face\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2, confidence = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    print(f\"Face {i+1}:\")\n",
    "    print(f\"  - Position: ({x1:.1f}, {y1:.1f}) to ({x2:.1f}, {y2:.1f})\")\n",
    "    print(f\"  - Size: {width:.1f} x {height:.1f} pixels\")\n",
    "    print(f\"  - Confidence: {confidence:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Explanation of the bounding box format:\n",
    "print(\"üìù Bounding box format: [x1, y1, x2, y2, confidence]\")\n",
    "print(\"   - (x1, y1): Top-left corner coordinates\")\n",
    "print(\"   - (x2, y2): Bottom-right corner coordinates\") \n",
    "print(\"   - confidence: How sure the detector is (0-1, higher = more confident)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressing 3DMM parameters, reconstruction and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: 3D FACE RECONSTRUCTION - PARAMETER REGRESSION ===\n",
    "# Now we use TDDFA to analyze each detected face and predict 3D parameters\n",
    "\n",
    "print(\"üß† Running 3D face analysis...\")\n",
    "print(\"This step:\")\n",
    "print(\"1. Crops each face region based on the detected bounding box\")\n",
    "print(\"2. Resizes to 120x120 pixels (neural network input size)\")  \n",
    "print(\"3. Runs the face through a neural network\")\n",
    "print(\"4. Outputs 62 parameters that describe the 3D face\")\n",
    "print()\n",
    "\n",
    "# Run 3DMM parameter regression\n",
    "param_lst, roi_box_lst = tddfa(img, boxes)\n",
    "\n",
    "print(f\"‚úÖ 3D analysis complete!\")\n",
    "print(f\"Generated parameters for {len(param_lst)} faces\")\n",
    "\n",
    "# Explain what we got back\n",
    "for i, (param, roi_box) in enumerate(zip(param_lst, roi_box_lst)):\n",
    "    print(f\"\\nFace {i+1} analysis results:\")\n",
    "    print(f\"  - 3DMM parameters: {len(param)} values\")\n",
    "    print(f\"    ‚Ä¢ First 12: Pose (rotation + translation)\")\n",
    "    print(f\"    ‚Ä¢ Next 40: Shape coefficients (face geometry)\")\n",
    "    print(f\"    ‚Ä¢ Last 10: Expression coefficients (facial expression)\")\n",
    "    print(f\"  - Refined bounding box: {[f'{x:.1f}' for x in roi_box]}\")\n",
    "    \n",
    "    # Show a few sample parameter values\n",
    "    print(f\"  - Sample pose values: {param[:3].round(3).tolist()}\")\n",
    "    print(f\"  - Sample shape values: {param[12:15].round(3).tolist()}\")\n",
    "    print(f\"  - Sample expression values: {param[52:55].round(3).tolist()}\")\n",
    "\n",
    "print(\"\\nüìö The 62 parameters are like 'DNA' for the face - they contain all\")\n",
    "print(\"   the information needed to reconstruct the 3D face shape!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6A: SPARSE LANDMARK RECONSTRUCTION ===\n",
    "# Convert the 62 parameters into actual 3D coordinates (sparse version)\n",
    "\n",
    "print(\"üìç Reconstructing SPARSE landmarks (68 key facial points)...\")\n",
    "print(\"Sparse landmarks include:\")\n",
    "print(\"- Face outline (17 points)\")  \n",
    "print(\"- Eyebrows (10 points)\")\n",
    "print(\"- Eyes (12 points)\")\n",
    "print(\"- Nose (9 points)\")\n",
    "print(\"- Mouth (20 points)\")\n",
    "print()\n",
    "\n",
    "# Reconstruct 3D vertices in sparse mode (68 landmarks only)\n",
    "dense_flag = False  # False = sparse landmarks, True = dense mesh\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)\n",
    "\n",
    "print(f\"‚úÖ Sparse reconstruction complete!\")\n",
    "for i, ver in enumerate(ver_lst):\n",
    "    print(f\"Face {i+1}: Generated {ver.shape[1]} landmark points\")\n",
    "    print(f\"  - 3D coordinates shape: {ver.shape} (3 rows: X,Y,Z coords)\")\n",
    "    print(f\"  - Sample coordinates: X={ver[0,0]:.1f}, Y={ver[1,0]:.1f}, Z={ver[2,0]:.1f}\")\n",
    "\n",
    "print(\"\\nüé® Drawing sparse landmarks on the image...\")\n",
    "result_img = draw_landmarks(img, ver_lst, dense_flag=dense_flag)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(result_img[..., ::-1])  # Convert BGR to RGB\n",
    "plt.title(\"Sparse Landmarks (68 Key Facial Points)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sparse landmark visualization complete!\")\n",
    "print(\"Each dot represents a key facial feature point in 3D space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6B: DENSE MESH RECONSTRUCTION ===\n",
    "# Convert the same 62 parameters into a DENSE 3D face mesh\n",
    "\n",
    "print(\"üï∏Ô∏è Reconstructing DENSE mesh (~38,000 vertices)...\")\n",
    "print(\"Dense reconstruction creates a complete 3D face surface with:\")\n",
    "print(\"- Every point on the face surface\")\n",
    "print(\"- Smooth transitions between features\") \n",
    "print(\"- Full geometric detail\")\n",
    "print(\"- Ready for 3D rendering and analysis\")\n",
    "print()\n",
    "\n",
    "# Reconstruct 3D vertices in dense mode (full mesh)\n",
    "dense_flag = True   # True = dense mesh (~38k points), False = sparse landmarks\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)\n",
    "\n",
    "print(f\"‚úÖ Dense reconstruction complete!\")\n",
    "for i, ver in enumerate(ver_lst):\n",
    "    print(f\"Face {i+1}: Generated {ver.shape[1]:,} mesh vertices\")\n",
    "    print(f\"  - 3D coordinates shape: {ver.shape}\")\n",
    "    print(f\"  - Memory usage: ~{ver.nbytes / 1024:.1f} KB per face\")\n",
    "    \n",
    "    # Show coordinate ranges to understand the 3D shape\n",
    "    x_range = ver[0].max() - ver[0].min()\n",
    "    y_range = ver[1].max() - ver[1].min() \n",
    "    z_range = ver[2].max() - ver[2].min()\n",
    "    print(f\"  - Face dimensions: {x_range:.1f} x {y_range:.1f} x {z_range:.1f} pixels\")\n",
    "\n",
    "print(\"\\nüé® Drawing dense landmarks on the image...\")\n",
    "result_img = draw_landmarks(img, ver_lst, dense_flag=dense_flag)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(result_img[..., ::-1])  # Convert BGR to RGB\n",
    "plt.title(\"Dense Landmarks (~38,000 Face Surface Points)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Dense landmark visualization complete!\")\n",
    "print(\"Notice how much more detailed the face surface representation is!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7: 3D MESH RENDERING ===\n",
    "# Render the 3D face mesh as a solid surface overlay on the original image\n",
    "\n",
    "print(\"üé≠ Rendering 3D face mesh overlay...\")\n",
    "print(\"This step:\")\n",
    "print(\"1. Takes the dense 3D vertices\")\n",
    "print(\"2. Connects them into triangular faces using mesh topology\")\n",
    "print(\"3. Renders the 3D surface with proper lighting and transparency\")\n",
    "print(\"4. Overlays the result on the original image\")\n",
    "print()\n",
    "\n",
    "# Reconstruct vertices for rendering (dense mesh required)\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)\n",
    "\n",
    "print(f\"üìê Mesh topology info:\")\n",
    "print(f\"  - Vertices: {ver_lst[0].shape[1]:,} points\")\n",
    "print(f\"  - Triangles: {len(tddfa.tri):,} faces\")\n",
    "print(f\"  - Each triangle connects 3 vertices to form a surface patch\")\n",
    "print()\n",
    "\n",
    "print(\"üé® Rendering 3D mesh...\")\n",
    "# Render the 3D mesh with semi-transparent overlay\n",
    "result_img = render(img, ver_lst, tddfa.tri, alpha=0.6, show_flag=True)\n",
    "\n",
    "# Note: The render function with show_flag=True will display the result automatically\n",
    "# and return the rendered image\n",
    "\n",
    "print(\"‚úÖ 3D mesh rendering complete!\")\n",
    "print()\n",
    "print(\"üîç What you're seeing:\")\n",
    "print(\"- The original image with a 3D face mesh overlay\")\n",
    "print(\"- Semi-transparent rendering (alpha=0.6) so you can see both\")\n",
    "print(\"- Proper 3D lighting and shading on the mesh surface\")\n",
    "print(\"- The mesh follows the exact contours of the detected face\")\n",
    "print()\n",
    "print(\"üéØ This proves the system has successfully:\")\n",
    "print(\"1. Detected the face in 2D\")\n",
    "print(\"2. Estimated the 3D pose and shape\") \n",
    "print(\"3. Reconstructed a complete 3D model\")\n",
    "print(\"4. Rendered it back onto the 2D image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 8: DEPTH MAP VISUALIZATION ===\n",
    "# Generate a depth map showing the 3D structure of the face\n",
    "\n",
    "print(\"üó∫Ô∏è Generating face depth map...\")\n",
    "print(\"A depth map visualizes 3D information by:\")\n",
    "print(\"- Showing distance from camera using colors/brightness\")\n",
    "print(\"- Closer parts appear brighter/warmer\")\n",
    "print(\"- Further parts appear darker/cooler\") \n",
    "print(\"- Reveals the 3D shape and structure of the face\")\n",
    "print()\n",
    "\n",
    "# Generate depth map from the 3D vertices\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)\n",
    "\n",
    "print(f\"üìä Depth analysis:\")\n",
    "for i, ver in enumerate(ver_lst):\n",
    "    z_coords = ver[2]  # Z-coordinates represent depth\n",
    "    min_depth = z_coords.min()\n",
    "    max_depth = z_coords.max()\n",
    "    depth_range = max_depth - min_depth\n",
    "    print(f\"Face {i+1} depth info:\")\n",
    "    print(f\"  - Closest point: {min_depth:.1f} pixels from camera\")\n",
    "    print(f\"  - Furthest point: {max_depth:.1f} pixels from camera\")\n",
    "    print(f\"  - Total depth range: {depth_range:.1f} pixels\")\n",
    "\n",
    "print(\"\\nüé® Rendering depth map...\")\n",
    "# Generate depth visualization\n",
    "result_img = depth(img, ver_lst, tddfa.tri, show_flag=True)\n",
    "\n",
    "# Note: The depth function with show_flag=True will display the result automatically\n",
    "\n",
    "print(\"‚úÖ Depth map generation complete!\")\n",
    "print()\n",
    "print(\"üîç How to read the depth map:\")\n",
    "print(\"- Bright/warm colors = closer to camera (nose tip, forehead)\")\n",
    "print(\"- Dark/cool colors = further from camera (eye sockets, sides)\")\n",
    "print(\"- The gradient shows the 3D curvature of facial features\")\n",
    "print(\"- This is similar to how 3D scanners represent depth information\")\n",
    "print()\n",
    "print(\"üéØ Applications of depth maps:\")\n",
    "print(\"- 3D face recognition and biometrics\")\n",
    "print(\"- Augmented reality face filters\")\n",
    "print(\"- Facial animation and motion capture\")\n",
    "print(\"- Medical and forensic face analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
